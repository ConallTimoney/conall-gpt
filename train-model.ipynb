{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9653981848502878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1112/1112 [01:04<00:00, 17.26 examples/s]\n",
      "Map: 100%|██████████| 279/279 [00:16<00:00, 17.14 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataCollatorForSeq2Seq.__init__() got an unexpected keyword argument 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m\n\u001b[0;32m     62\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m data: tokenize_data(data, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m data: tokenize_data(data, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 66\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[43mDataCollatorForSeq2Seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_pad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     73\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     77\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m     78\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m     79\u001b[0m \tauto_find_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     use_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     88\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: DataCollatorForSeq2Seq.__init__() got an unexpected keyword argument 'truncation'"
     ]
    }
   ],
   "source": [
    "from utils.data import create_train_valid_whatsapp\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    "    , DataCollatorForSeq2Seq\n",
    "    , AutoTokenizer\n",
    "    , Seq2SeqTrainer\n",
    "    , Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "\n",
    "MODEL_ID = 't5-small'\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = get_peft_model(\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, device_map='auto')\n",
    "    , lora_config\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# we need to get the maxium input and output token lengths \n",
    "\n",
    "def tokenize_data(data_set, tokenizer):\n",
    "    model_inputs = tokenizer(\n",
    "        data_set['text']\n",
    "        , truncation=True\n",
    "        , padding=True\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        data_set['text']\n",
    "        , truncation=True\n",
    "        , padding=True\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "data = create_train_valid_whatsapp(\n",
    "    path='./raw-data/karamh chat.txt'\n",
    "    , train_sender_name='Conall'\n",
    "    , prompt_max_length=tokenizer.model_max_length\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenized_train = data['train'].map(lambda data: tokenize_data(data, tokenizer=tokenizer), batched=False, remove_columns=['text', 'label'])\n",
    "tokenized_test = data['test_data'].map(lambda data: tokenize_data(data, tokenizer=tokenizer), batched=False, remove_columns=['text', 'label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer\n",
    "    , model=model\n",
    "    , label_pad_token_id=tokenizer.pad_token_id\n",
    "    , pad_to_multiple_of=8\n",
    "    , padding=True\n",
    ")\n",
    "\n",
    "output_dir='test'\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/695 [10:08<7:10:31, 38.04s/it]\n",
      "  0%|          | 0/695 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  6%|▋         | 45/695 [16:57<4:07:17, 22.83s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conall-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
